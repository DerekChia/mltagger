{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "This repo is based on the paper [Zero-shot Sequence Labeling: Transferring Knowledge from Sentences to Tokens](https://arxiv.org/pdf/1805.02214.pdf) by Marek Rei and Anders SÃ¸gaard. The original code repositary can be found [here](https://github.com/marekrei/mltagger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To begin, train the model with `experiment.py` where it takes in a configuration file that contains the necessary hyperparameters and file paths. The script is dependent on two other files ([model.py](https://github.com/DerekChia/mltagger/blob/master/src/model.py) and [evaluator.py](https://github.com/DerekChia/mltagger/blob/master/src/evaluator.py)) where the MLTModel and MLTEvaluator objects are initialised and updated during runtime. Since we are using Jupyter Notebook, these two files are imported using `model.ipynb` and `evaluator.ipynb`.\n",
    "\n",
    "Reference: [src/experiment.py](https://github.com/DerekChia/mltagger/blob/master/src/experiment.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run model.ipynb\n",
    "%run evaluator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import numpy\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import ConfigParser as configparser\n",
    "except:\n",
    "    import configparser\n",
    "\n",
    "# Replaced by %run model.ipynb and evaluator.ipynb\n",
    "# from model import MLTModel\n",
    "# from evaluator import MLTEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available functions in experiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "def read_input_files(file_paths, max_sentence_length=-1):\n",
    "    return sentences\n",
    "\n",
    "def parse_config(config_section, config_path):\n",
    "    return config\n",
    "\n",
    "def is_float(value):\n",
    "    return {True, False}\n",
    "\n",
    "def create_batches_of_sentence_ids(sentences, batch_equal_size, max_batch_size):\n",
    "    return batches_of_sentence_ids\n",
    "\n",
    "def process_sentences(epoch, data, model, is_training, learningrate, config, name):\n",
    "    return results \n",
    "\n",
    "def run_experiment(config_path)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read_input_files\n",
    "This function takes in the input file(s) path and return sentences in the following format. Note that the input files are expected to be in TSV format, CoNLL style.\n",
    "\n",
    "```\n",
    "    sentence = [['Not', 'c'], ['only', 'c'], ['as', 'c'], ['a', 'c'], ['hobby', 'c'], ['.', 'c']]\n",
    "```\n",
    "\n",
    "Return:\n",
    "```\n",
    "    sentences = [\n",
    "    [['Not', 'c'], ['only', 'c'], ['as', 'c'], ['a', 'c'], ['hobby', 'c'], ['.', 'c']],\n",
    "    [['They', 'c'], ['use', 'c'], ['computers', 'c'], ['for', 'c'], ['their', 'c'], ['works', 'i'], ['.', 'c']]\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_files(file_paths, max_sentence_length=-1):\n",
    "    \"\"\"\n",
    "    Reads input files in whitespace-separated format.\n",
    "    Will split file_paths on comma, reading from multiple files.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    line_length = None\n",
    "    # file_path might contain multiple files, split them and iterate through\n",
    "    for file_path in file_paths.strip().split(\",\"):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            sentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                # Ensure that line contains character, else this is an indicator for newline\n",
    "                if len(line) > 0:\n",
    "                    line_parts = line.split()\n",
    "                    # Check if input file has both word and label\n",
    "                    # Might not be a necessary check during inference since \n",
    "                    # input may not have a ground truth (second column)\n",
    "                    # assert(len(line_parts) >= 2), line\n",
    "                    # assert(len(line_parts) == line_length or line_length == None)\n",
    "                    # line_length = len(line_parts)\n",
    "                    sentence.append(line_parts)\n",
    "                # If line has no character (i.e. empty) and length of previous sentence is more than zero,\n",
    "                # push the previous sentence into the sentences list and anticipate new sentence.\n",
    "                elif len(line) == 0 and len(sentence) > 0:\n",
    "                    if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                        sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            # Not in use\n",
    "            if len(sentence) > 0:\n",
    "                if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                    sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_config\n",
    "This function reads configuration from the input `config_path` and returns a dictionary where it tries to guess the correct datatype for each config value. `config_section` is used as a starting key [config]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(config_section, config_path):\n",
    "    \"\"\"\n",
    "    Reads configuration from the file and returns a dictionary.\n",
    "    Tries to guess the correct datatype for each of the config values.\n",
    "    \"\"\"\n",
    "    config_parser = configparser.SafeConfigParser(allow_no_value=True)\n",
    "    config_parser.read(config_path)\n",
    "    config = collections.OrderedDict()\n",
    "    for key, value in config_parser.items(config_section):\n",
    "        if value is None or len(value.strip()) == 0:\n",
    "            config[key] = None\n",
    "        elif value.lower() in [\"true\", \"false\"]:\n",
    "            config[key] = config_parser.getboolean(config_section, key)\n",
    "        elif value.isdigit():\n",
    "            config[key] = config_parser.getint(config_section, key)\n",
    "        elif is_float(value):\n",
    "            config[key] = config_parser.getfloat(config_section, key)\n",
    "        else:\n",
    "            config[key] = config_parser.get(config_section, key)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is_float\n",
    "This function checks if the parameter is of type float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(value):\n",
    "    \"\"\"\n",
    "    Check in value is of type float()\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_batches_of_sentence_ids\n",
    "Based on the input parameters:\n",
    "\n",
    "- If `batch_equal_size` is `True`, group sentences into batches by IDs based on their length, according to a limit of `max_batch_size`. For example, if there are 64 sentences with length 10 (i.e. 64 sentences each with 10 characters in total) and `max_batch_size` is set to `32`, these 64 sentences will be batched together into 2 batches.\n",
    "    ```\n",
    "    Example:\n",
    "    Length of each sentence, [ID of each sentence]\n",
    "    10, [   [0, ... , 31]\n",
    "    10,     [32, ... , 63]\n",
    "    25,     [2553, 2680]\n",
    "    43,     [142, 305, 1490, 1775, 1973]\n",
    "    27,     [194, 197, 245, 348, 359]\n",
    "    XX,     [...]    ]\n",
    "    ```\n",
    "- If `batch_equal_size` is `False`, each batch will contain a fixed number of sentences (`max_batch_size`).\n",
    "    ```\n",
    "    Example:\n",
    "    [ID of each sentence]\n",
    "    [\n",
    "        [0, 1, 2, 3, 4]\n",
    "        [5, 6, 7, 8, 9]\n",
    "        [10, 11, 12, 13, 14]\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "Return: \n",
    "- List of lists with sentences ids.\n",
    "\n",
    "Default value:\n",
    "- `batch_equal_size` = `True`\n",
    "- `max_batch_size` = `32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches_of_sentence_ids(sentences, batch_equal_size, max_batch_size):\n",
    "    \"\"\"\n",
    "    Groups together sentences into batches\n",
    "    If max_batch_size is positive, this value determines the maximum number of sentences in each batch.\n",
    "    If max_batch_size has a negative value, the function dynamically creates the batches such that each batch contains abs(max_batch_size) words.\n",
    "    Returns a list of lists with sentences ids.\n",
    "    \"\"\"\n",
    "    if batch_equal_size == True:\n",
    "        sentence_ids_by_length = collections.OrderedDict()\n",
    "        for _id, sentence in enumerate(sentences):\n",
    "            length = len(sentence)\n",
    "            if length not in sentence_ids_by_length:\n",
    "                sentence_ids_by_length[length] = []\n",
    "                sentence_ids_by_length[length].append(_id)\n",
    "            else:\n",
    "                sentence_ids_by_length[length].append(_id)\n",
    "        # Fitting each batch to the size defined in max_batch_size\n",
    "        for sentence_length in sentence_ids_by_length:\n",
    "            for i in range(0, len(sentence_ids_by_length[sentence_length]), max_batch_size):\n",
    "                batches_of_sentence_ids.append(sentence_ids_by_length[sentence_length][i : i + max_batch_size])\n",
    "    else:\n",
    "        batch = []\n",
    "        for i in range(len(sentences)):\n",
    "            if len(batch) != max_batch_size:\n",
    "                batch.append(i)\n",
    "            else:\n",
    "                batches_of_sentence_ids.append(batch)\n",
    "                batch = []\n",
    "                batch.append(i)\n",
    "    return batches_of_sentence_ids\n",
    "\n",
    "\"\"\"Original Code Below\"\"\"\n",
    "#     batches_of_sentence_ids = []\n",
    "#     if batch_equal_size == True:\n",
    "#         sentence_ids_by_length = collections.OrderedDict()\n",
    "#         sentence_length_sum = 0.0\n",
    "#         for i in range(len(sentences)):\n",
    "#             length = len(sentences[i])\n",
    "#             if length not in sentence_ids_by_length:\n",
    "#                 sentence_ids_by_length[length] = []\n",
    "#             sentence_ids_by_length[length].append(i)\n",
    "\n",
    "#         for sentence_length in sentence_ids_by_length:\n",
    "#             if max_batch_size > 0:\n",
    "#                 batch_size = max_batch_size\n",
    "#             else:\n",
    "#                 batch_size = int((-1 * max_batch_size) / sentence_length)\n",
    "\n",
    "#             for i in range(0, len(sentence_ids_by_length[sentence_length]), batch_size):\n",
    "#                 batches_of_sentence_ids.append(sentence_ids_by_length[sentence_length][i:i + batch_size])\n",
    "#     else:\n",
    "#         current_batch = []\n",
    "#         max_sentence_length = 0\n",
    "#         for i in range(len(sentences)):\n",
    "#             current_batch.append(i)\n",
    "#             if len(sentences[i]) > max_sentence_length:\n",
    "#                 max_sentence_length = len(sentences[i])\n",
    "#             if (max_batch_size > 0 and len(current_batch) >= max_batch_size) \\\n",
    "#               or (max_batch_size <= 0 and len(current_batch)*max_sentence_length >= (-1 * max_batch_size)):\n",
    "#                 batches_of_sentence_ids.append(current_batch)\n",
    "#                 current_batch = []\n",
    "#                 max_sentence_length = 0\n",
    "#         if len(current_batch) > 0:\n",
    "#             batches_of_sentence_ids.append(current_batch)\n",
    "#     return batches_of_sentence_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_sentences\n",
    "\n",
    "Function processes all sentences with `evaluator` object and return evaluation metrics. Function will trigger `process_batch` function by feeding in batch of sentences and learning rate configured.\n",
    "\n",
    "Return:\n",
    "- `evaluator` object with results populated\n",
    "\n",
    "Default values:\n",
    "- `garbage_collection` = `False`\n",
    "- `batch_equal_size` = `True`\n",
    "- `max_batch_size` = `32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(epoch, data, model, is_training, learningrate, config, name):\n",
    "    \"\"\"\n",
    "    Process all the sentences with the labeler, return evaluation metrics.\n",
    "    \"\"\"\n",
    "    evaluator = MLTEvaluator(config)\n",
    "    \n",
    "    # From data, create batches of sentences ids based batch_equal_size flag.\n",
    "    # batch_equal_size = False, max_batch_size = 32\n",
    "    batches_of_sentence_ids = create_batches_of_sentence_ids(data, config[\"batch_equal_size\"], config[\"max_batch_size\"])\n",
    "    \n",
    "    # Randomly shuffle data AGAIN if this is training data. \n",
    "    # Question: Why shuffle again when data_train was shuffled in run_experiment() before this?\n",
    "    # See random.shuffle(data_train)\n",
    "    if is_training == True:\n",
    "        random.shuffle(batches_of_sentence_ids)\n",
    "    \n",
    "    # sentence_ids_in_batch refers to each batch of sentence_ids\n",
    "    # E.g. sentence_ids_in_batch = [10, 11, 12, 13, 14]\n",
    "    for count, sentence_ids_in_batch in enumerate(batches_of_sentence_ids):        \n",
    "        # Build batch from data. Each batch now contains sentences (not ID)\n",
    "        # For every sentence ID (i) in sentence_ids, get the actual sentence data[i]\n",
    "        # batch = [\n",
    "        # [['Not', 'c'], ['only', 'c'], ['as', 'c'], ['a', 'c'], ['hobby', 'c'], ['.', 'c']], # First sentence\n",
    "        # [['They', 'c'], ['use', 'c'], ['computers', 'c'], ['for', 'c'], ['their', 'c'], ['works', 'i'], ['.', 'c']] # Second sentence\n",
    "        #]\n",
    "        batch = [data[i] for i in sentence_ids_in_batch]\n",
    "        \n",
    "        # Get cost, predicted labels and probs for each batch\n",
    "        print('############### Epoch', epoch + 1,'Batch', count + 1, 'of', len(batches_of_sentence_ids) , '###############')\n",
    "        cost, sentence_scores, token_scores_list = model.process_batch(batch, is_training, learningrate)\n",
    "        \n",
    "        # Append cost, predicted labels and probs to the evaluator object\n",
    "        evaluator.append_data(cost, batch, sentence_scores, token_scores_list)\n",
    "    \n",
    "        # Not in use. garbage_collection defaults to False.\n",
    "        while config[\"garbage_collection\"] == True and gc.collect() > 0:\n",
    "            pass\n",
    "\n",
    "        results = evaluator.get_results(name)\n",
    "        for key in results:\n",
    "            print(key + \": \" + str(results[key]))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_experiment\n",
    "\n",
    "This function puts together all the supporting functions (above). \n",
    "1. Load train, dev and test data using read_input_files\n",
    "2. Intialize MLTModel with configuration\n",
    "3. \n",
    "\n",
    "\n",
    "Dependencies:\n",
    "- model\n",
    "- model.build_vocabs\n",
    "- model.construct_network\n",
    "- model.initialize_session\n",
    "- model.preload_word_embeddings\n",
    "- model.get_parameter_count\n",
    "- model.get_parameter_count_without_word_embeddings\n",
    "\n",
    "Default values (config):\n",
    "- random_seed = 100\n",
    "- {path_train, path_dev, path_test} = PATH_TO_DATA\n",
    "- preload_vectors = glove.6B.300d.txt\n",
    "- model_selector = dev_sentence_f1_score:high\n",
    "- learningrate = 1.0\n",
    "- epochs = 200\n",
    "- stop_if_no_improvement_for_epochs = 7\n",
    "- learningrate_decay = 0.9\n",
    "- save = PATH_TO_SAVED_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config_path):\n",
    "    # config is an ConfigParser object\n",
    "    config = parse_config(\"config\", config_path)\n",
    "    \n",
    "    # Temporary path for storing model\n",
    "    temp_model_path = config_path + \".model\"\n",
    "    \n",
    "    # Taking in the random seed from config\n",
    "    if \"random_seed\" in config:\n",
    "        random.seed(config[\"random_seed\"])\n",
    "        numpy.random.seed(config[\"random_seed\"])\n",
    "\n",
    "    # Not needed - To print everything in config\n",
    "    # for key, val in config.items():\n",
    "    #     print(str(key) + \": \" + str(val))\n",
    "\n",
    "    # Turning training, dev and test dataset into sentences using read_input_files\n",
    "    data_train, data_dev, data_test = None, None, None\n",
    "    if config[\"path_train\"] != None and len(config[\"path_train\"]) > 0:\n",
    "        data_train = read_input_files(config[\"path_train\"], config[\"max_train_sent_length\"])\n",
    "    if config[\"path_dev\"] != None and len(config[\"path_dev\"]) > 0:\n",
    "        data_dev = read_input_files(config[\"path_dev\"])\n",
    "    \n",
    "    # There could be multiple test datasets, so we need to iterate through the paths\n",
    "    if config[\"path_test\"] != None and len(config[\"path_test\"]) > 0:\n",
    "        data_test = []\n",
    "        for path_test in config[\"path_test\"].strip().split(\":\"):\n",
    "            data_test += read_input_files(path_test)\n",
    "    \n",
    "    # Initialising the model object\n",
    "    model = MLTModel(config)\n",
    "    \n",
    "    # build_vocabs returns a list of vocabulary in a form of word2id, char2id and singletons\n",
    "    model.build_vocabs(data_train, data_dev, data_test, config[\"preload_vectors\"])\n",
    "    \n",
    "    # construct_network will set up tensorflow graph with the layers described in the paper\n",
    "    model.construct_network()\n",
    "    \n",
    "    # Initialising tf.Session and set up session configuration\n",
    "    model.initialize_session()\n",
    "    if config[\"preload_vectors\"] != None:\n",
    "        model.preload_word_embeddings(config[\"preload_vectors\"])\n",
    "    \n",
    "    # Get the number of parameters that need to be tuned in the graph\n",
    "    print(\"parameter_count: \" + str(model.get_parameter_count()))\n",
    "    print(\"parameter_count_without_word_embeddings: \" + str(model.get_parameter_count_without_word_embeddings()))\n",
    "\n",
    "    # Run if there is training data loaded\n",
    "    if data_train != None:\n",
    "        # model_selector = dev_sentence_f1_score\n",
    "        # model_selector_type = high\n",
    "        model_selector = config[\"model_selector\"].split(\":\")[0]\n",
    "        model_selector_type = config[\"model_selector\"].split(\":\")[1]\n",
    "        best_selector_value = 0.0\n",
    "        best_epoch = -1\n",
    "        learningrate = config[\"learningrate\"]\n",
    "        \n",
    "        # This is where training begins - iterating through the epochs\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            print(\"EPOCH: \" + str(epoch))\n",
    "            print(\"current_learningrate: \" + str(learningrate))\n",
    "            \n",
    "            # Shuffling training data (the first time). Will be shuffling again in process_sentences\n",
    "            random.shuffle(data_train)\n",
    "            \n",
    "            # process_sentences will trigger \n",
    "            results_train = process_sentences(epoch, data_train, model, is_training=True, learningrate=learningrate, config=config, name=\"train\")\n",
    "\n",
    "            # Run if there is dev data loaded\n",
    "            if data_dev != None:\n",
    "                results_dev = process_sentences(epoch, data_dev, model, is_training=False, learningrate=0.0, config=config, name=\"dev\")\n",
    "\n",
    "                if math.isnan(results_dev[\"dev_cost_sum\"]) or math.isinf(results_dev[\"dev_cost_sum\"]):\n",
    "                    raise ValueError(\"Cost is NaN or Inf. Exiting.\")\n",
    "\n",
    "                if (epoch == 0 or (model_selector_type == \"high\" and results_dev[model_selector] > best_selector_value) \n",
    "                               or (model_selector_type == \"low\" and results_dev[model_selector] < best_selector_value)):\n",
    "                    best_epoch = epoch\n",
    "                    best_selector_value = results_dev[model_selector]\n",
    "                    model.saver.save(model.session, temp_model_path, latest_filename=os.path.basename(temp_model_path)+\".checkpoint\")\n",
    "                print(\"best_epoch: \" + str(best_epoch))\n",
    "\n",
    "                if config[\"stop_if_no_improvement_for_epochs\"] > 0 and (epoch - best_epoch) >= config[\"stop_if_no_improvement_for_epochs\"]:\n",
    "                    break\n",
    "\n",
    "                if (epoch - best_epoch) > 3:\n",
    "                    learningrate *= config[\"learningrate_decay\"]\n",
    "\n",
    "            while config[\"garbage_collection\"] == True and gc.collect() > 0:\n",
    "                pass\n",
    "\n",
    "        if data_dev != None and best_epoch >= 0:\n",
    "            # loading the best model so far\n",
    "            model.saver.restore(model.session, temp_model_path)\n",
    "            os.remove(temp_model_path+\".checkpoint\")\n",
    "            os.remove(temp_model_path+\".data-00000-of-00001\")\n",
    "            os.remove(temp_model_path+\".index\")\n",
    "            os.remove(temp_model_path+\".meta\")\n",
    "\n",
    "    if config[\"save\"] is not None and len(config[\"save\"]) > 0:\n",
    "        model.save(config[\"save\"])\n",
    "\n",
    "    if config[\"path_test\"] is not None:\n",
    "        i = 0\n",
    "        for path_test in config[\"path_test\"].strip().split(\":\"):\n",
    "            data_test = read_input_files(path_test)\n",
    "            results_test = process_sentences(epoch, data_test, model, is_training=False, learningrate=0.0, config=config, name=\"test\"+str(i))\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main\n",
    "\n",
    "Executes run_experiment with configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_experiment('config.conf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
