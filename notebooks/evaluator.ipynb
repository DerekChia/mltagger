{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import collections\n",
    "import numpy\n",
    "\n",
    "class MLTEvaluator(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.cost_sum = 0.0\n",
    "        self.sentence_count = 0.0\n",
    "        self.sentence_correct_binary = 0.0\n",
    "        self.sentence_predicted = 0.0\n",
    "        self.sentence_correct = 0.0\n",
    "        self.sentence_total = 0.0\n",
    "\n",
    "        self.token_ap_sum = []\n",
    "        self.token_predicted = []\n",
    "        self.token_correct = []\n",
    "        self.token_total = []\n",
    "\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def calculate_ap(self, true_labels, predicted_scores):\n",
    "        assert(len(true_labels) == len(predicted_scores))\n",
    "        indices = numpy.argsort(numpy.array(predicted_scores))[::-1]\n",
    "        summed, correct, total = 0.0, 0.0, 0.0\n",
    "        for index in indices:\n",
    "            total += 1.0\n",
    "            if true_labels[index] >= 0.5:\n",
    "                correct += 1.0\n",
    "                summed += correct / total\n",
    "        return (summed / correct) if correct > 0.0 else 0.0\n",
    "\n",
    "\n",
    "    def append_token_data_for_sentence(self, index, true_labels, token_scores):\n",
    "        if len(self.token_ap_sum) <= index:\n",
    "            self.token_ap_sum.append(0.0)\n",
    "            self.token_predicted.append(0.0)\n",
    "            self.token_correct.append(0.0)\n",
    "            self.token_total.append(0.0)\n",
    "\n",
    "        ap = self.calculate_ap(true_labels, token_scores[:len(true_labels)])\n",
    "        self.token_ap_sum[index] += ap\n",
    "\n",
    "        for i in range(len(true_labels)):\n",
    "            if true_labels[i] >= 0.5:\n",
    "                self.token_total[index] += 1.0\n",
    "            if token_scores[i] >= 0.5:\n",
    "                self.token_predicted[index] += 1.0\n",
    "            if true_labels[i] >= 0.5 and token_scores[i] >= 0.5:\n",
    "                self.token_correct[index] += 1.0\n",
    "\n",
    "\n",
    "    def append_data(self, cost, batch, sentence_scores, token_scores_list):\n",
    "        assert(len(self.token_ap_sum) == 0 or len(self.token_ap_sum) == len(token_scores_list))\n",
    "        # Summing up all costs across batches\n",
    "        self.cost_sum += cost\n",
    "\n",
    "        # Added\n",
    "        total_count_interesting_labels = []\n",
    "\n",
    "        # Evaluate each sentence (i) in batch\n",
    "        for i in range(len(batch)):\n",
    "            self.sentence_count += 1.0\n",
    "\n",
    "            # Ground Truth\n",
    "            true_labels = [1.0 if batch[i][j][-1] != self.config[\"default_label\"] else 0.0 for j in range(len(batch[i]))]\n",
    "            count_interesting_labels = numpy.array(true_labels).sum()\n",
    "\n",
    "            # Added\n",
    "            # print(count_interesting_labels)\n",
    "            # print( [batch[i][j][-1] for j in range(len(batch[i]))] )\n",
    "            # print(true_labels)\n",
    "            # print( 'Sentence', i, sentence_scores[i], [batch[i][j][0] for j in range(len(batch[i]))] )\n",
    "\n",
    "            total_count_interesting_labels.append(count_interesting_labels)\n",
    "\n",
    "            if (count_interesting_labels == 0.0 and sentence_scores[i] < 0.5) or (count_interesting_labels > 0.0 and sentence_scores[i] >= 0.5):\n",
    "                self.sentence_correct_binary += 1.0\n",
    "\n",
    "            # For every sentence that is more than 0.5, it is correctly predicted?\n",
    "            if sentence_scores[i] >= 0.5:\n",
    "                self.sentence_predicted += 1.0\n",
    "\n",
    "            if count_interesting_labels > 0.0:\n",
    "                self.sentence_total += 1.0\n",
    "\n",
    "            if count_interesting_labels > 0.0 and sentence_scores[i] >= 0.5:\n",
    "                self.sentence_correct += 1.0\n",
    "\n",
    "            for k in range(len(token_scores_list)):\n",
    "                self.append_token_data_for_sentence(k, true_labels, token_scores_list[k][i])\n",
    "\n",
    "        # print('Interesting Labels', total_count_interesting_labels)\n",
    "\n",
    "\n",
    "    # def get_results(self, name):\n",
    "    #     p = (float(self.sentence_correct) / float(self.sentence_predicted)) if (self.sentence_predicted > 0.0) else 0.0\n",
    "    #     r = (float(self.sentence_correct) / float(self.sentence_total)) if (self.sentence_total > 0.0) else 0.0\n",
    "    #     f = (2.0 * p * r / (p + r)) if (p+r > 0.0) else 0.0\n",
    "    #     f05 = ((1.0 + 0.5*0.5) * p * r / ((0.5*0.5 * p) + r)) if (((0.5*0.5 * p) + r) > 0.0) else 0.0\n",
    "\n",
    "    #     results = collections.OrderedDict()\n",
    "    #     results[name + \"_cost_sum\"] = self.cost_sum\n",
    "    #     results[name + \"_cost_avg\"] = self.cost_sum / float(self.sentence_count)\n",
    "    #     results[name + \"_sent_count\"] = self.sentence_count\n",
    "    #     results[name + \"_sent_predicted\"] = self.sentence_predicted\n",
    "    #     results[name + \"_sent_correct\"] = self.sentence_correct\n",
    "    #     results[name + \"_sent_total\"] = self.sentence_total\n",
    "    #     results[name + \"_sent_p\"] = p\n",
    "    #     results[name + \"_sent_r\"] = r\n",
    "    #     results[name + \"_sent_f\"] = f\n",
    "    #     results[name + \"_sent_f05\"] = f05\n",
    "    #     results[name + \"_sent_correct_binary\"] = self.sentence_correct_binary\n",
    "    #     results[name + \"_sent_accuracy_binary\"] = self.sentence_correct_binary / float(self.sentence_count)\n",
    "\n",
    "    #     for k in range(len(self.token_ap_sum)):\n",
    "    #         mean_ap = self.token_ap_sum[k] / self.sentence_total # only calculating MAP over sentences that have any positive tokens\n",
    "    #         p = (float(self.token_correct[k]) / float(self.token_predicted[k])) if (self.token_predicted[k] > 0.0) else 0.0\n",
    "    #         r = (float(self.token_correct[k]) / float(self.token_total[k])) if (self.token_total[k] > 0.0) else 0.0\n",
    "    #         f = (2.0 * p * r / (p + r)) if (p+r > 0.0) else 0.0\n",
    "    #         f05 = ((1.0 + 0.5*0.5) * p * r / ((0.5*0.5 * p) + r)) if (((0.5*0.5 * p) + r) > 0.0) else 0.0\n",
    "\n",
    "    #         results[name + \"_tok_\"+str(k)+\"_map\"] = mean_ap\n",
    "    #         results[name + \"_tok_\"+str(k)+\"_p\"] = p\n",
    "    #         results[name + \"_tok_\"+str(k)+\"_r\"] = r\n",
    "    #         results[name + \"_tok_\"+str(k)+\"_f\"] = f\n",
    "    #         results[name + \"_tok_\"+str(k)+\"_f05\"] = f05\n",
    "\n",
    "    #     results[name + \"_time\"] = float(time.time()) - float(self.start_time)\n",
    "\n",
    "    #     return results\n",
    "\n",
    "    def get_results(self, name):\n",
    "        precision = (float(self.sentence_correct) / float(self.sentence_predicted)) if (self.sentence_predicted > 0) else 0.0\n",
    "        recall = (float(self.sentence_correct) / float(self.sentence_predicted) if (self.sentence_predicted > 0) else 0.0)\n",
    "        # https://en.wikipedia.org/wiki/F1_score\n",
    "        f1_score = (2.0 * precision * recall / (precision + recall)) if (precision + recall > 0.0) else 0.0\n",
    "        # f0.5 score weighs recall lower than precision (by attenuating the influence of false negatives)\n",
    "        f05_score = ((1.0 + 0.5 * 0.5) * (precision * recall) / (0.5 * 0.5 * precision + recall)) if (precision + recall > 0.0) else 0.0\n",
    "\n",
    "        results = collections.OrderedDict()\n",
    "        # Average cost of each word generated in this Epoch\n",
    "        results[name + '_cost_avg'] = self.cost_sum / float(self.sentence_count)\n",
    "        # Total cost generated in this Epoch by summing all loss across each word\n",
    "        results[name + '_cost_sum'] = self.cost_sum\n",
    "\n",
    "        results[name + \"_sentence_count\"] = self.sentence_count\n",
    "        results[name + \"_sentence_predicted\"] = self.sentence_predicted\n",
    "        results[name + \"_sentence_correct\"] = self.sentence_correct\n",
    "        results[name + \"_sentence_total\"] = self.sentence_total\n",
    "        # Precision\n",
    "        results[name + '_sentence_precision'] = precision\n",
    "        # Recall\n",
    "        results[name + '_sentence_recall'] = recall\n",
    "        # F1 Score\n",
    "        results[name + '_sentence_f1_score'] = f1_score\n",
    "        # F05 Score\n",
    "        results[name + '_sentence_f05_score'] = f05_score\n",
    "        \n",
    "        results[name + \"_sentence_correct_binary\"] = self.sentence_correct_binary\n",
    "        results[name + \"_sentence_accuracy_binary\"] = self.sentence_correct_binary / float(self.sentence_count)\n",
    "\n",
    "        for k in range(len(self.token_ap_sum)):\n",
    "            mean_ap = self.token_ap_sum[k] / self.sentence_total # only calculating MAP over sentences that have any positive tokens\n",
    "            p = (float(self.token_correct[k]) / float(self.token_predicted[k])) if (self.token_predicted[k] > 0.0) else 0.0\n",
    "            r = (float(self.token_correct[k]) / float(self.token_total[k])) if (self.token_total[k] > 0.0) else 0.0\n",
    "            f = (2.0 * p * r / (p + r)) if (p+r > 0.0) else 0.0\n",
    "            f05 = ((1.0 + 0.5*0.5) * p * r / ((0.5*0.5 * p) + r)) if (((0.5*0.5 * p) + r) > 0.0) else 0.0\n",
    "\n",
    "            results[name + \"_tok_\"+str(k)+\"_map\"] = mean_ap\n",
    "            results[name + \"_tok_\"+str(k)+\"_p\"] = p\n",
    "            results[name + \"_tok_\"+str(k)+\"_r\"] = r\n",
    "            results[name + \"_tok_\"+str(k)+\"_f\"] = f\n",
    "            results[name + \"_tok_\"+str(k)+\"_f05\"] = f05\n",
    "\n",
    "        results[name + \"_time\"] = float(time.time()) - float(self.start_time)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
